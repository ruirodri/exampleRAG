# exampleRAG
Baseado no artigo que pode ser encontrado no link abaixo:

https://duyhuynh.substack.com/p/build-your-own-rag-and-run-it-locally

Este é um exemplo de aplicação preparada para conversar com um LLM local (um Mistral, no caso) rodando sob Ollama, e que vai utilizar uma abordagem de Retrieval Augmented Generation (RAG) para adicionar conteúdos em documentos de texto como contexto para a geração de respostas do LLM.


